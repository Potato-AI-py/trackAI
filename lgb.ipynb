{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-10T13:45:53.097578Z",
     "iopub.status.busy": "2023-03-10T13:45:53.096476Z",
     "iopub.status.idle": "2023-03-10T13:45:53.879567Z",
     "shell.execute_reply": "2023-03-10T13:45:53.877396Z",
     "shell.execute_reply.started": "2023-03-10T13:45:53.097530Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-03-22T01:43:04.148758Z",
     "end_time": "2023-03-22T01:43:04.337233Z"
    }
   },
   "outputs": [],
   "source": [
    "KAGGLE = False #if getpass.getuser() == \"Kaihua\" else True\n",
    "\n",
    "import os, copy, zipfile, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from functools import reduce\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-22T01:43:04.158747Z",
     "end_time": "2023-03-22T01:43:04.353165Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [],
   "source": [
    "#!pip3 install lightgbm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-03-22T01:43:04.180780Z",
     "end_time": "2023-03-22T01:43:04.362145Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-10T13:45:53.887735Z",
     "iopub.status.busy": "2023-03-10T13:45:53.887208Z",
     "iopub.status.idle": "2023-03-10T13:45:55.077345Z",
     "shell.execute_reply": "2023-03-10T13:45:55.075607Z",
     "shell.execute_reply.started": "2023-03-10T13:45:53.887685Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-03-22T01:43:04.196108Z",
     "end_time": "2023-03-22T01:43:04.362145Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "DATA_PATH = ''\n",
    "\n",
    "\n",
    "SEED = 2024\n",
    "cat_feats = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-22T01:43:04.228024Z",
     "end_time": "2023-03-22T01:43:13.599302Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: (791616, 77)\n",
      "test_data: (216384, 74)\n",
      "all: (1008000, 80)\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_parquet(os.path.join(DATA_PATH, 'track3_train.parquet'))\n",
    "test_data = pd.read_parquet(os.path.join(DATA_PATH, 'track3_a.parquet'))\n",
    "\n",
    "train_data['type'] = 'train'\n",
    "test_data['type'] = 'test'\n",
    "data = pd.concat([train_data, test_data], axis=0).reset_index(drop=True)\n",
    "data = pd.concat([data, data['ID'].str.split('_', expand=True).rename(columns={0:'station',1:'sample',2:'time'})], axis=1)#细化向关联项\n",
    "\n",
    "print(f'train_data: {train_data.shape}')\n",
    "print(f'test_data: {test_data.shape}')\n",
    "print(f'all: {data.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-22T01:43:13.607806Z",
     "end_time": "2023-03-22T01:43:13.656684Z"
    }
   },
   "outputs": [],
   "source": [
    "TARGET_FEATS = ['wdir_2min', 'spd_2min', 'spd_inst_max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-22T01:43:13.617792Z",
     "end_time": "2023-03-22T01:43:13.689595Z"
    }
   },
   "outputs": [],
   "source": [
    "# 删除有问题的目标\n",
    "for item in TARGET_FEATS:\n",
    "    idx = data[item] >= 199999.0\n",
    "    data.loc[idx, item] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-22T01:43:13.669649Z",
     "end_time": "2023-03-22T01:43:16.036252Z"
    }
   },
   "outputs": [],
   "source": [
    "data['station'] = data['station'].apply(lambda x: int(x.split('D')[-1]))\n",
    "cat_feats += ['station']\n",
    "\n",
    "data['sample'] = data['sample'].apply(lambda x: int(x.split('Sample')[-1]))\n",
    "data['time'] = data['time'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-22T01:43:16.044223Z",
     "end_time": "2023-03-22T01:43:16.052200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feats: 74, ['100u', '100v', '10u', '10v', '2d', '2t', 'cape', 'capes', 'cp', 'deg0l', 'lcc', 'msl', 'skt', 'sp', 'sst', 'tcc', 'd_L1000', 'q_L1000', 'r_L1000', 't_L1000', 'u_L1000', 'v_L1000', 'w_L1000', 'd_L950', 'q_L950', 'r_L950', 't_L950', 'u_L950', 'v_L950', 'w_L950', 'd_L925', 'q_L925', 'r_L925', 't_L925', 'u_L925', 'v_L925', 'w_L925', 'd_L900', 'q_L900', 'r_L900', 't_L900', 'u_L900', 'v_L900', 'w_L900', 'd_L850', 'q_L850', 'r_L850', 't_L850', 'u_L850', 'v_L850', 'w_L850', 'd_L700', 'q_L700', 'r_L700', 't_L700', 'u_L700', 'v_L700', 'w_L700', 'd_L500', 'q_L500', 'r_L500', 't_L500', 'u_L500', 'v_L500', 'w_L500', 'd_L200', 'q_L200', 'r_L200', 't_L200', 'u_L200', 'v_L200', 'w_L200', 'station', 'time']\n",
      "['station']\n"
     ]
    }
   ],
   "source": [
    "feats = [item for item in data.columns if item not in TARGET_FEATS+['ID', 'sample', 'type']]#剔除不相关项\n",
    "print(f'feats: {len(feats)}, {feats}')\n",
    "print(cat_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [
    {
     "data": {
      "text/plain": "                           ID  wdir_2min  spd_2min  spd_inst_max      100u  \\\n0        D0011_Sample00000_01       88.0       4.5           5.2 -3.178867   \n1        D0011_Sample00000_02      101.0       4.5           6.0 -3.044837   \n2        D0011_Sample00000_03      111.0       3.9           5.1 -2.910807   \n3        D0011_Sample00000_04      160.0       1.7           4.3 -1.870547   \n4        D0011_Sample00000_05      179.0       1.3           2.9 -0.830286   \n...                       ...        ...       ...           ...       ...   \n1007995  D0009_Sample02128_44        NaN       NaN           NaN  3.843052   \n1007996  D0009_Sample02128_45        NaN       NaN           NaN  3.674979   \n1007997  D0009_Sample02128_46        NaN       NaN           NaN  4.163034   \n1007998  D0009_Sample02128_47        NaN       NaN           NaN  4.651088   \n1007999  D0009_Sample02128_48        NaN       NaN           NaN  5.139143   \n\n             100v       10u       10v        2d        2t  ...    q_L200  \\\n0        1.520631 -3.035558  0.921220 -4.296475  2.166330  ...  0.005066   \n1        1.737420 -2.924957  1.208521 -3.802847  2.137752  ...  0.005006   \n2        1.954209 -2.814357  1.495822 -3.309219  2.109175  ...  0.004946   \n3        1.442103 -1.843161  1.229792 -3.500122  1.850405  ...  0.004919   \n4        0.929998 -0.871964  0.963762 -3.691025  1.591636  ...  0.004893   \n...           ...       ...       ...       ...       ...  ...       ...   \n1007995  8.813500  3.456899  8.086399 -6.134135  4.715720  ...  0.005002   \n1007996  9.288052  3.270604  8.504519 -6.362700  5.081217  ...  0.005217   \n1007997  9.299423  3.684067  8.492985 -5.268479  5.118454  ...  0.005117   \n1007998  9.310794  4.097529  8.481452 -4.174258  5.155692  ...  0.005016   \n1007999  9.322165  4.510991  8.469918 -3.080037  5.192929  ...  0.004915   \n\n           r_L200     t_L200     u_L200    v_L200    w_L200   type  station  \\\n0        7.282292 -54.574743  60.027509  4.458829  0.052047  train     1101   \n1        6.950891 -54.490239  58.961815  3.799950  0.053916  train     1102   \n2        6.619490 -54.405734  57.896121  3.141070  0.055785  train     1103   \n3        6.931894 -54.718634  57.157805  2.129127  0.064269  train     1104   \n4        7.244298 -55.031533  56.419490  1.117184  0.072752  train     1105   \n...           ...        ...        ...       ...       ...    ...      ...   \n1007995  7.706795 -54.882592  58.070532 -1.666188  0.017625   test      944   \n1007996  8.115371 -54.979291  58.097675 -2.525959 -0.009414   test      945   \n1007997  7.934781 -54.952597  58.558392 -3.415087  0.020155   test      946   \n1007998  7.754191 -54.925903  59.019110 -4.304215  0.049725   test      947   \n1007999  7.573601 -54.899210  59.479827 -5.193343  0.079294   test      948   \n\n         sample  time  \n0             0     1  \n1             0     2  \n2             0     3  \n3             0     4  \n4             0     5  \n...         ...   ...  \n1007995    2128    44  \n1007996    2128    45  \n1007997    2128    46  \n1007998    2128    47  \n1007999    2128    48  \n\n[1008000 rows x 80 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>wdir_2min</th>\n      <th>spd_2min</th>\n      <th>spd_inst_max</th>\n      <th>100u</th>\n      <th>100v</th>\n      <th>10u</th>\n      <th>10v</th>\n      <th>2d</th>\n      <th>2t</th>\n      <th>...</th>\n      <th>q_L200</th>\n      <th>r_L200</th>\n      <th>t_L200</th>\n      <th>u_L200</th>\n      <th>v_L200</th>\n      <th>w_L200</th>\n      <th>type</th>\n      <th>station</th>\n      <th>sample</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>D0011_Sample00000_01</td>\n      <td>88.0</td>\n      <td>4.5</td>\n      <td>5.2</td>\n      <td>-3.178867</td>\n      <td>1.520631</td>\n      <td>-3.035558</td>\n      <td>0.921220</td>\n      <td>-4.296475</td>\n      <td>2.166330</td>\n      <td>...</td>\n      <td>0.005066</td>\n      <td>7.282292</td>\n      <td>-54.574743</td>\n      <td>60.027509</td>\n      <td>4.458829</td>\n      <td>0.052047</td>\n      <td>train</td>\n      <td>1101</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>D0011_Sample00000_02</td>\n      <td>101.0</td>\n      <td>4.5</td>\n      <td>6.0</td>\n      <td>-3.044837</td>\n      <td>1.737420</td>\n      <td>-2.924957</td>\n      <td>1.208521</td>\n      <td>-3.802847</td>\n      <td>2.137752</td>\n      <td>...</td>\n      <td>0.005006</td>\n      <td>6.950891</td>\n      <td>-54.490239</td>\n      <td>58.961815</td>\n      <td>3.799950</td>\n      <td>0.053916</td>\n      <td>train</td>\n      <td>1102</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>D0011_Sample00000_03</td>\n      <td>111.0</td>\n      <td>3.9</td>\n      <td>5.1</td>\n      <td>-2.910807</td>\n      <td>1.954209</td>\n      <td>-2.814357</td>\n      <td>1.495822</td>\n      <td>-3.309219</td>\n      <td>2.109175</td>\n      <td>...</td>\n      <td>0.004946</td>\n      <td>6.619490</td>\n      <td>-54.405734</td>\n      <td>57.896121</td>\n      <td>3.141070</td>\n      <td>0.055785</td>\n      <td>train</td>\n      <td>1103</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>D0011_Sample00000_04</td>\n      <td>160.0</td>\n      <td>1.7</td>\n      <td>4.3</td>\n      <td>-1.870547</td>\n      <td>1.442103</td>\n      <td>-1.843161</td>\n      <td>1.229792</td>\n      <td>-3.500122</td>\n      <td>1.850405</td>\n      <td>...</td>\n      <td>0.004919</td>\n      <td>6.931894</td>\n      <td>-54.718634</td>\n      <td>57.157805</td>\n      <td>2.129127</td>\n      <td>0.064269</td>\n      <td>train</td>\n      <td>1104</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>D0011_Sample00000_05</td>\n      <td>179.0</td>\n      <td>1.3</td>\n      <td>2.9</td>\n      <td>-0.830286</td>\n      <td>0.929998</td>\n      <td>-0.871964</td>\n      <td>0.963762</td>\n      <td>-3.691025</td>\n      <td>1.591636</td>\n      <td>...</td>\n      <td>0.004893</td>\n      <td>7.244298</td>\n      <td>-55.031533</td>\n      <td>56.419490</td>\n      <td>1.117184</td>\n      <td>0.072752</td>\n      <td>train</td>\n      <td>1105</td>\n      <td>0</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1007995</th>\n      <td>D0009_Sample02128_44</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.843052</td>\n      <td>8.813500</td>\n      <td>3.456899</td>\n      <td>8.086399</td>\n      <td>-6.134135</td>\n      <td>4.715720</td>\n      <td>...</td>\n      <td>0.005002</td>\n      <td>7.706795</td>\n      <td>-54.882592</td>\n      <td>58.070532</td>\n      <td>-1.666188</td>\n      <td>0.017625</td>\n      <td>test</td>\n      <td>944</td>\n      <td>2128</td>\n      <td>44</td>\n    </tr>\n    <tr>\n      <th>1007996</th>\n      <td>D0009_Sample02128_45</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.674979</td>\n      <td>9.288052</td>\n      <td>3.270604</td>\n      <td>8.504519</td>\n      <td>-6.362700</td>\n      <td>5.081217</td>\n      <td>...</td>\n      <td>0.005217</td>\n      <td>8.115371</td>\n      <td>-54.979291</td>\n      <td>58.097675</td>\n      <td>-2.525959</td>\n      <td>-0.009414</td>\n      <td>test</td>\n      <td>945</td>\n      <td>2128</td>\n      <td>45</td>\n    </tr>\n    <tr>\n      <th>1007997</th>\n      <td>D0009_Sample02128_46</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4.163034</td>\n      <td>9.299423</td>\n      <td>3.684067</td>\n      <td>8.492985</td>\n      <td>-5.268479</td>\n      <td>5.118454</td>\n      <td>...</td>\n      <td>0.005117</td>\n      <td>7.934781</td>\n      <td>-54.952597</td>\n      <td>58.558392</td>\n      <td>-3.415087</td>\n      <td>0.020155</td>\n      <td>test</td>\n      <td>946</td>\n      <td>2128</td>\n      <td>46</td>\n    </tr>\n    <tr>\n      <th>1007998</th>\n      <td>D0009_Sample02128_47</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4.651088</td>\n      <td>9.310794</td>\n      <td>4.097529</td>\n      <td>8.481452</td>\n      <td>-4.174258</td>\n      <td>5.155692</td>\n      <td>...</td>\n      <td>0.005016</td>\n      <td>7.754191</td>\n      <td>-54.925903</td>\n      <td>59.019110</td>\n      <td>-4.304215</td>\n      <td>0.049725</td>\n      <td>test</td>\n      <td>947</td>\n      <td>2128</td>\n      <td>47</td>\n    </tr>\n    <tr>\n      <th>1007999</th>\n      <td>D0009_Sample02128_48</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5.139143</td>\n      <td>9.322165</td>\n      <td>4.510991</td>\n      <td>8.469918</td>\n      <td>-3.080037</td>\n      <td>5.192929</td>\n      <td>...</td>\n      <td>0.004915</td>\n      <td>7.573601</td>\n      <td>-54.899210</td>\n      <td>59.479827</td>\n      <td>-5.193343</td>\n      <td>0.079294</td>\n      <td>test</td>\n      <td>948</td>\n      <td>2128</td>\n      <td>48</td>\n    </tr>\n  </tbody>\n</table>\n<p>1008000 rows × 80 columns</p>\n</div>"
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-03-22T01:43:16.059186Z",
     "end_time": "2023-03-22T01:43:16.615735Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-22T01:43:16.622233Z",
     "end_time": "2023-03-22T01:43:17.994647Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "train_data = data.query('type==\"train\"').reset_index(drop=True)\n",
    "test_data = data.query('type==\"test\"').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-03-22T01:43:18.000611Z",
     "end_time": "2023-03-22T01:43:18.011587Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "lgb_params = {  # 用于控制模型训练的超参数字典，如学习率、树的深度、子采样比例等\n",
    "    'boosting_type': 'gbdt', # 默认参数，基于梯度提升决策树的方法，可以用于回归和分类问题，速度较快、准确度较高\n",
    "    'objective': 'quantile', #mse mape\n",
    "    'metric': 'quantile',\n",
    "    # 'max_depth': 6,\n",
    "    'num_leaves': 2 ** 6, # 默认31  越大模型复杂度越高易过拟合，反之欠拟合\n",
    "    # 'num_leaves': 31,\n",
    "    'min_data_in_leaf': 20,  # TODO 参数调小，这样可以提高模型的复杂度，提高模型的拟合能力\n",
    "#     'lambda_l1': 0.5,  \n",
    "#     'lambda_l2': 0.5,  \n",
    "    'feature_fraction': 0.7,   #每一次树木生长时选择多少特征。常用的取值范围是0.5到1\n",
    "    'bagging_fraction': 0.7,  #控制在每次迭代中，模型将使用多少数据。0.5到1是一个常见的取值范围\n",
    "    'bagging_freq': 5,  # 每n次迭代进行子采样，减少子采样的频率可以提高准确性，但会增加训练时间。默认为0\n",
    "    'learning_rate': 0.01,  #取值范围一般是0.001到1之间 较小的学习率会导致模型需要更多的 迭代次数 才能收敛，但更大的学习率可能会导致模型不收敛或过拟合。一般调小\n",
    "    'n_jobs': -1,\n",
    "    'verbose': -1,\n",
    "    \"device_type\": \"cpu\",\n",
    "    'feature_fraction_seed':SEED, #控制特征选择过程中的随机种子，用于生成不同的特征\n",
    "    'bagging_seed':SEED, #控制bagging操作的随机种子，用于生成不同的训练样本\n",
    "    'seed': SEED, #控制数据的随机排序和划分。默认为0，即每次运行时都会产生相同的结果\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-22T01:43:18.015579Z",
     "end_time": "2023-03-22T01:43:18.055047Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 逐小时预报采用四舍五入保留1位小数位\n",
    "# 24小时内最大风由保留1位小数位的逐小时平均风速求最大值\n",
    "# 24小时内极大风由保留1位小数位的逐小时极大风速求最大值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-22T01:43:18.031120Z",
     "end_time": "2023-03-22T01:43:18.055047Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "task_name = \"lgb\"\n",
    "task_params = {\"lgb\": lgb_params}[task_name] # 用于控制模型训练的超参数字典，如学习率、树的深度、子采样比例等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train_oof = {}\n",
    "test_pred = {}\n",
    "feats_importance = {}\n",
    "\n",
    "for item_target in TARGET_FEATS:\n",
    "    train_y = train_data[item_target]\n",
    "    idx = ~train_y.isna()\n",
    "\n",
    "    # trian_id = train_data.loc[idx, 'ID'].reset_index(drop=True)  #原始训练集中ID列+为缺失值的结果列\n",
    "    train_x = train_data.loc[idx, feats].reset_index(drop=True)  #训练集（已细化并剔除不相关）+为缺失值的结果列\n",
    "    testA_x = test_data[feats].reset_index(drop=True) #测试集（已细化并剔除不相关）\n",
    "    train_y = train_y.loc[idx].reset_index(drop=True) #缺失值的结果列\n",
    "    group_x = train_data.loc[idx, 'sample'].reset_index(drop=True) #原始训练集中sample列+为缺失值的结果列\n",
    "    print(train_x.shape, testA_x.shape)\n",
    "\n",
    "    # item_oof = np.zeros(train_x.shape[0])\n",
    "    item_pred = np.zeros(testA_x.shape[0])\n",
    "\n",
    "    fold_num = 5\n",
    "    item_importance = 0\n",
    "    from sklearn.model_selection import GroupKFold #用于分组数据的交叉验证迭代器 测试模型的泛化能力 检测过拟合情况\n",
    "    import lightgbm as lgb #轻量级梯度提升  基于学习算法的决策树  分布式、高效\n",
    "    kf = GroupKFold(n_splits=fold_num, ) #n_splits一般3-10之间  数据集越大n_splits调小  越大越精准但泛化能力下降过拟合增大   需要找平衡点\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(train_x, groups=group_x)): #基于GroupKFold交叉验证分组，根据sample标识（文件夹）分组  保证每个分组样本均匀分配到不同训练集和验证集中\n",
    "        print('-----------', fold)\n",
    "        train = lgb.Dataset(   # 数据集封装成一个对象、加载入内存 方便LightGBM 进行训练和预测\n",
    "            train_x.loc[train_idx], #训练集\n",
    "            train_y.loc[train_idx],\n",
    "            categorical_feature=cat_feats # 类别特征列表，用于指定数据集中哪些特征是类别型的特征\n",
    "        )\n",
    "        val = lgb.Dataset(\n",
    "            train_x.loc[val_idx], #验证集\n",
    "            train_y.loc[val_idx],\n",
    "            categorical_feature=cat_feats # cat_feats=['station']\n",
    "        )\n",
    "        model = lgb.train(task_params, train, valid_sets=[train, val], num_boost_round=90_000,\n",
    "                            callbacks=[lgb.early_stopping(100), lgb.log_evaluation(100)])\n",
    "        #valid_sets 验证数据集为什么还要加上train\n",
    "        \"\"\"\n",
    "        params: 用于控制模型训练的超参数字典，如学习率、树的深度、子采样比例等。\n",
    "        train_set: 训练数据集，可以是 lgb.Dataset 对象或数据矩阵。\n",
    "        num_boost_round: 迭代次数，即弱分类器的数量。\n",
    "        valid_sets: 验证数据集，可以是一个 lgb.Dataset 对象或一个列表，其中每个元素都是 lgb.Dataset 对象。\n",
    "        early_stopping_rounds: 早停次数，如果在连续的 early_stopping_rounds 次迭代中验证集的损失没有得到改善，则停止迭代。\n",
    "            lgb.early_stopping()比这个好用，可以把num_boost_round放的很大，他可以智能判断早停\n",
    "        callbacks: 回调函数列表，用于在训练过程中执行特定操作，如打印日志、保存模型等。\n",
    "        \"\"\"\n",
    "        # item_oof[val_idx] += (model.predict(train_x.loc[val_idx])) #预测验证集结果存入item_oof\n",
    "\n",
    "        item_pred += (model.predict(testA_x))/fold_num  #预测测试集/折数=平局值 item_pred\n",
    "        item_importance += model.feature_importance(importance_type='gain') / fold_num # 特征性/折数=平均值 importance_type=split/gain 分别表示特征被用于分裂的次数和特征对模型的贡献度\n",
    "\n",
    "    importance = pd.DataFrame()\n",
    "    importance['name'] = feats\n",
    "    importance['importance'] = item_importance\n",
    "\n",
    "    # train_oof[item_target] = pd.DataFrame({\"ID\": trian_id, f\"{item_target}_true\": train_y, f\"{item_target}_pred\": item_oof}) #重构预测的验证集结果列\n",
    "    test_pred[item_target] = pd.DataFrame({\"ID\": test_data['ID'], f\"{item_target}\": item_pred})\n",
    "    feats_importance[item_target] = importance #重构预测的测试集结果列"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train_oof = reduce(lambda x,y: pd.merge(x,y,on='ID', how='outer'), train_oof.values())\n",
    "test_pred = reduce(lambda x,y: pd.merge(x,y,on='ID', how='outer'), test_pred.values())\n",
    "# train_oof.shape\n",
    "test_pred.shape\n",
    "\n",
    "score_str = 'potato_90_stationTime'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"result\"):\n",
    "    os.makedirs(\"result\")\n",
    "# train_oof.to_csv(os.path.join(\"result\", f'lgb_oof_{score_str}.csv'), index=False)\n",
    "test_pred.to_csv(os.path.join(\"result\", f'{score_str}.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('AL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "82785e3617b41da40eac9dc72b5795aea1d455d121b27bd2aa7c1fc59bb3871c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
